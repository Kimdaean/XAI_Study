# ì˜ˆì œ 7.1 í•©ì„±ê³± ì‹ ê²½ë§ì„ ì œì‘í•˜ê³  ì •í™•ë„ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.contrib.slim as slim
# slimì€ CNNì„ ë§Œë“¤ ë•Œ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ ì„ ì–¸ ë“±ì„ ì¶”ìƒí™”í•  ìˆ˜ ìˆëŠ” ê³ ìˆ˜ì¤€ ê²½ëŸ‰ API
from tensorflow.examples.tutorials.mnist import input_data

# extracting MNIST Dataset
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# construct CNN
# set models tf.reset_default_graph()
# placeholderë€, ì²˜ìŒì— ë³€ìˆ˜ë¥¼ ì„ ì–¸í•  ë•Œ ê°’ì„ ë°”ë¡œ ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë‚˜ì¤‘ì— ê°’ì„ ë˜ì ¸ì£¼ëŠ” 'ê³µê°„'ì„ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒ
x = tf.placeholder(tf.float32, [None, 784],name="x-in") # 28 x 28 x 1 MNIST ë°ì´í„°ë¥¼ í‰íƒ„í™”(1ì°¨ì› ë°°ì—´ë¡œ ë°”ê¿”ì¤Œ)
# ìˆœì„œëŒ€ë¡œ ë°ì´í„° íƒ€ì…, ì…ë ¥ ë°ì´í„°ì˜ í˜•íƒœ, í•´ë‹¹ placeholderì˜ ì´ë¦„ ì„¤ì •
true_y = tf.placeholder(tf.float32, [None, 10],name="y-in")
# true_yì—ëŠ” 0 ~ 9ê¹Œì§€ì˜ ë¼ë²¨ì´ ë“¤ì–´ê°
keep_prob = tf.placeholder(dtype=tf.float32)
# tf.placeholder("float") -> ì˜ˆì „ ë²„ì „ì´ì–´ì„œ ê°€ëŠ¥, í˜„ì¬ëŠ” tf.placeholder(dtype=tf.float32) ì´ë ‡ê²Œ ì ì–´ì•¼ í•¨
x_image = tf.reshape(x,[-1,28,28,1]) #í‰íƒ„í™”í•œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 28 x 28 x 1ë¡œ ë°”ê¿”ì¤Œ
# x_image = tf.reshape(x,[-1,784]) #í‰íƒ„í™”í•œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 28 x 28 x 1ë¡œ ë°”ê¿”ì¤Œ
# print(x_image.shape)
# print(x_image)

# layer 1
hidden_1 = slim.conv2d(x_image,5,[5,5])
# print(hidden_1.shape)
# print(hidden_1)
# exit()
pool_1 = slim.max_pool2d(hidden_1,[2,2])
# print(pool_1.shape)

# layer 2
hidden_2 = slim.conv2d(pool_1,5,[5,5])
# print(hidden_2.shape)
pool_2 = slim.max_pool2d(hidden_2,[2,2])
# print(pool_2.shape)

# layer 3
hidden_3 = slim.conv2d(pool_2,20,[5,5])
#print(hidden_3.shape)
hidden_3 = slim.dropout(hidden_3,keep_prob)
print(hidden_3.shape)
#print()

out_y = slim.fully_connected(slim.flatten(hidden_3), 10, activation_fn=tf.nn.softmax)

cross_entropy = -tf.reduce_sum(true_y*tf.log(out_y))
correct_prediction = tf.equal(tf.argmax(out_y,1), tf.argmax(true_y,1))
# tf.equal í•¨ìˆ˜ëŠ” ì¸ìë¡œ ë°›ì€ ë‘ í…ì„œì˜ ì›ì†Œê°€ ê°™ì„ ê²½ìš°, Trueë¥¼ ê°™ì§€ ì•Šë‹¤ë©´ Falseë¥¼ ë°˜í™˜
# tf.argmax(a,0) , tf.argmax(a,1) -> 0ì´ë©´ ê° 'ì—´'ì—ì„œ ê°€ì¥ í° ê°’ì„ ì°¾ì•„ í•´ë‹¹ ê°’ì˜ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ë°˜í™˜. 1ì´ë©´ ê° 'í–‰'ì—ì„œ ê°€ì¥ í° ê°’ì„ ì°¾ì•„ í•´ë‹¹ ê°’ì˜ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ë°˜í™˜
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
# tf.cast í•¨ìˆ˜ëŠ” ë¶€ë™ì†Œìˆ˜ì í˜•ì—ì„œ ì†Œìˆ˜í˜•ìœ¼ë¡œ ë°”ê¾¸ê³  ì†Œìˆ˜ì ì„ ë²„ë¦¼
# ex) 1.89999 -> 0 , 2.09999 -> 1
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

# learning
batchSize = 50
sess = tf.Session()
# ì„¸ì…˜ì„ ë§Œë“¤ê³  ì—°ì‚°ê·¸ë˜í”„ ì‹¤í–‰
init = tf.global_variables_initializer() # ë³€ìˆ˜ ì´ˆê¸°í™”
sess.run(init) # ë³€ìˆ˜ ì´ˆê¸°í™” ìˆ˜í–‰

for i in range(1000):
    batch = mnist.train.next_batch(batchSize) # batchsizeë§Œí¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í•™ìŠµì„ ì§„í–‰
    sess.run(train_step, feed_dict={x:batch[0],true_y:batch[1], keep_prob:0.5})
    # xì— batch[0]ì„ ë„£ê³ , yì— batch[1]ì„ ë„£ëŠ”ë‹¤. keep_prob:0.5ëŠ” 0.5ë§Œí¼ í•™ìŠµí•˜ëŠ”ë° ì‚¬ìš©
    if i % 100 == 0 and i != 0:
        trainAccuracy = sess.run(accuracy, feed_dict={x:batch[0], true_y:batch[1], keep_prob:1.0})
        print("step %d, training accuracy %g"%(i, trainAccuracy))
        # print(f"out_y shpae:{out_y.shape}")

# print Accuracy
print('Accuracy: {:.2%}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, true_y: mnist.test.labels, keep_prob: 1.0})))

# ì˜ˆì œ 7.2 í•„ìš” ë³€ìˆ˜ 1, 2ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ
# í•„ìš”í•œ ë³€ìˆ˜ë“¤
# 1. ì‹ ê²½ë§ì˜ ê³„ì¸µë³„ í™œì„±í™” í•¨ìˆ˜
# 2. ì‹ ê²½ë§ì˜ ë‘ ê³„ì¸µ ì‚¬ì´ë¥¼ ì‡ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°
# 3. ë¶„ë¥˜ ê²°ê³¼ê°’ = í•´ë‹¹ ì½”ë“œì—ì„œì˜ out_y
# 4. ì‹ ê²½ë§ì˜ ì• ê³„ì¸µ ë…¸ë“œë³„ íƒ€ë‹¹ì„± ìˆ˜ì¹˜ëŠ” ë¶„ë¥˜ ê²°ê³¼ê°’ìœ¼ë¡œë¶€í„° ë‘ ê³„ì¸µ ì‚¬ì´ë¥¼ ì‡ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ê°€ê³µí•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤
layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]
for i in layers:
    print(i.shape)

print('*'*10,'layers','*'*10)
for layer in layers:
    print("###############layer:",layer)
# get_collection ë©”ì„œë“œëŠ” í…ì„œí”Œë¡œ ì„¸ì…˜ì— ì˜¬ë¼ê°„ ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ë‹¤. get_collection ëª…ë ¹ì–´ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ë°”ì´ì–´ìŠ¤ë¥¼ ëª¨ë‘ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
# get_collectionì€ í˜„ì¬ í…ì„œí”Œë¡œê°€ ì„¸ì…˜ì— ì˜¬ë ¤ë†“ì€ ê·¸ë˜í”„ ì¤‘ íŠ¹ì • ë³€ìˆ«ê°’ì„ ë¶ˆëŸ¬ ì˜¤ëŠ” ë©”ì„œë“œ
weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*weights.*')
# ê°€ì¤‘ì¹˜(weigths)ì™€ ë°”ì´ì–´ìŠ¤(bias)ëŠ” scope ë³€ìˆ˜ë¥¼ ì¡°ì •í•´ì„œ í•„í„°ë§í•œë‹¤
print('*'*10,'weights','*'*10)
for weight in weights:
    print(f"{weight}:{weight.shape}")
biases = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*biases.*')
print('*'*10,'biases','*'*10)
for bias in biases:
    print(f"{bias}:{bias}")
# bias ë˜í•œ ë§ˆì°¬ê°€ì§€

# ì˜ˆì œ 6.14(ì¤‘ë³µ) ê³„ì¸µë³„ í™œì„±í™” í•¨ìˆ˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ
def getActivations(layer, image):
    units = sess.run(layer, feed_dict={x: np.reshape(image, [1,784], order='F'), keep_prob:1.0})
    # print(f"#####################units:{units.shape}")
    return units

# ì˜ˆì œ 7.4 LRPë¥¼ ìˆ˜í–‰í•  ì´ë¯¸ì§€ í•˜ë‚˜ë¥¼ ë¶€ë¥´ëŠ” ê³¼ì •

# show test image
idx = 4
imageToUse = mnist.test.images[idx]
plt.imshow(np.reshape(imageToUse,[28,28]), interpolation="nearest", cmap='gray')

# ì˜ˆì œ 7.5 í•©ì„±ê³± ì‹ ê²½ë§ì˜ ì€ë‹‰ ê³„ì¸µë§ˆë‹¤ í™œì„±í™” í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ
# layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]
activations = []
for layer in layers:
    print(layer)
    activations.append(getActivations(layer, imageToUse))
# print('#'*15,'4ë²ˆì§¸ ì‚¬ì§„','#'*15)
# a=1
# for i in activations:
#     print(a)
#     print(i)
#     print("ê¸¸ì´:",len(i))
#     a+=1
# print("activations[0]: ",activations[0])
# print('#'*15)
# print(len(activations))

# ì˜ˆì œ 7.6 í•©ì„±ê³± ì‹ ê²½ë§ì— ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•˜ê³  out_y ì˜ˆì¸¡ ê²°ê³¼ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ
predict = sess.run(out_y, feed_dict={x: imageToUse.reshape([-1, 784]), keep_prob:1.0})
print(predict)
predict = sess.run(out_y, feed_dict={x: imageToUse.reshape([-1, 784]), keep_prob:1.0})[0]
idx = 0
for i in predict:
    print('[{}] {:.2%}'.format(idx, i))
    idx += 1
print(predict)
# ì˜ˆì œ 7.7 í•™ìŠµëœ í•©ì„±ê³± ì‹ ê²½ë§ìœ¼ë¡œë¶€í„° ë¶„ë¥˜ ê°€ëŠ¥ì„±ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ

f_x = max(predict)
# print("f_x:",f_x)
# print(type(f_x))

#######í•©ì„±ê³± ì‹ ê²½ë§ì— LRP ì ìš©í•˜ê¸°

# ì˜ˆì œ 7.8 ì™„ì „ ì—°ê²° ì‹ ê²½ë§ì—ì„œ ì—­ì „íŒŒ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ìˆ˜ë„ ì½”ë“œ
# get FC layer gradient
def getGradient(activation, weight, bias):
    print("#"*20,"getGradient","#"*20)
    print("activation: ",activation)
    # forward pass
    W = tf.maximum(0., weight)
    print("Weight : ",W)
    b = tf.maximum(0., bias)
    print("Bias : ", b)
    z = tf.matmul(activation, w) + b
    print("Z : ",z)
    # backward pass
    dX = tf.matmul(1/z, tf.transpose(W))
    print("dX : ",dX)
    return dX

#  ì˜ˆì œ 7.9 f_xë¡œë¶€í„° ë°”ë¡œ ì§ì „ ì€ë‹‰ì¸µì˜ íƒ€ë‹¹ì„± ì „íŒŒ ê°’ì„ êµ¬í•˜ëŠ” ì½”ë“œ

R4 = predict
print(f"R4 shape:\n{R4.shape}")
print(f"R4:\n{R4}")

# ì˜ˆì œ 7.9 FC ì—°ê²°ì—ì„œ LRPë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ. ì˜ˆì œ 7.8ì˜ ì—­ì „íŒŒ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œì— íƒ€ë‹¹ì„± ë³€ìˆ˜(relevance)ë¥¼ ê³±í•œë‹¤

def backprop_dense(activation, weight, bias, relevance):
    print("#"*20,"backprop_dense","#"*20)
    print("activation: ",activation)
    print("relevance: ",relevance)
    w = tf.maximum(0., weight)
    print("weight: ",w)
    print(f"weight shape: {weight.shape}")
    b = tf.maximum(0., bias)
    print("bias: ",b)
    print(f"bias shape: {bias.shape}")
    z = tf.matmul(activation, w) + b
    print("z: ",z)
    print(f"z shpae: {z.shape}")
    s = relevance / z
    print("s: ",s)
    print(f"s shape: {s.shape}")
    print(f"transpose w: {tf.transpose(w).shape}")
    c = tf.matmul(s, tf.transpose(w))
    print("c: ",c)
    print(f"c shape: {c.shape}")
    return activation * c

# ì˜ˆì œ 7.10 ì˜ˆì œ 7.9ì—ì„œ ë§Œë“  LRP ê³µì‹ìœ¼ë¡œ ğ‘…3ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ

# layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]
# (1, 28, 28, 5) (1, 14, 14, 5) (1, 7, 7, 20)
# activation, weights, biases
a = activations.pop()
# print(f"a type:\n{type(a)}")
# print(f"a len:\n{a.shape}")
# print(f"a:\n{a}")
w = weights.pop()
# print(f"w type:\n{type(w)}")
# print(f"w len:\n{w.shape}")
# print(f"w:\n{w}")
b = biases.pop()
# print(f"b type:\n{type(b)}")
# print(f"b:\n{b}")

# print(f"a.shape: {a.shape}")
# print(f"w.shape: {w.shape}")

R3 = backprop_dense(a.reshape(1,980), w, b, R4)
print(R3)
print(f"R3.shape: {R3.shape}")
exit()


# ì˜ˆì œ 7.11-(1) ì–¸í’€ë§ ì—°ì‚°ì—ì„œ LRPë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ
from tensorflow.python.ops import gen_nn_ops

def backprop_pooling(activation, relevance):
    # kernel size, strides
    # if z is zero
    ksize = strides = [1, 2, 2, 1]
    z = tf.nn.max_pool(activation, ksize, strides, padding='SAME') + 1e-10
    s = relevance / z
    # input, argmax, argmax_mask
    c = gen_nn_ops._max_pool_grad(activation, z, s, ksize, strides, padding='SAME')
    print(f"c;\n{c}")
    return activation * c

# ì˜ˆì œ 7.11-(2) ì—­í•©ì„±ê³± ì—°ì‚°ì—ì„œ LRPë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ

def backprop_conv(activation, weight, bias, relevance):
    strides = [1, 1, 1, 1]
    w = tf.maximum(0., weight)
    b = tf.maximum(0., bias)
    z = tf.nn.conv2d(activation, w, strides, padding='SAME')
    z = tf.nn.bias_add(z, b)
    print(f"z:\n{z}")
    s = relevance / z
    c = tf.nn.conv2d_backprop_input(tf.shape(activation), w, s, strides, padding='SAME')
    return activation * c

# ì˜ˆì œ 7.12 ğ‘…3 ë²¡í„°ë¡œë¶€í„° ì—­í•©ì„±ê³±ê³¼ ì–¸í’€ë§ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ğ‘…2 ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ

# layers = [hidden_1, pool_1, hidden_2, pool_2]
# (1, 28, 28, 5)(1, 14, 14, 5)(1, 7, 7, 20)
# activation, weights, biases
w = weights.pop()
b = biases.pop()
p = activations.pop()
a = activations.pop()
print(p.shape)

# convolution backprop
R_conv = backprop_conv(p, w, b, tf.reshape(R3, [1, 7, 7, 20]))
print(R_conv.shape)
R2 = backprop_pooling(a, R_conv)
print(R2.shape)

# ì˜ˆì œ 7.11 ğ‘…2ì—ì„œ ì—­í•©ì„±ê³±ê³¼ ì–¸í’€ë§ ê³¼ì •ì„ ìˆ˜í–‰í•˜ê³  ğ‘…1 ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œ

# layers = [hidden_1, pool_1]
# (1, 28, 28, 5)(1, 14, 14, 5)
# activation, weights, biases
w = weights.pop()
b = biases.pop()
p = activations.pop()
a = activations.pop()

# convolution backprop
R_conv = backprop_conv(p, w, b, R2)
print(R_conv.shape)

R1 = backprop_pooling(a, R_conv)
print(R1.shape)
print(np.sum(sess.run(R1)))

# ì˜ˆì œ 7.12 ğ‘…1 ê²°ê³¼ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ê¹Œì§€ LRPë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ

img_activations = getActivations(x_image, imageToUse)
w = weights.pop()
b = biases.pop()
R0 = backprop_conv(img_activations, w, b, R1)
print(f"R0:\n{R0}")
LRP_out = sess.run(R0)
print(f"LRP out:\n{LRP_out}")


# ì˜ˆì œ 7.13 ì›ë³¸ ì´ë¯¸ì§€ í˜•íƒœë¡œ íƒ€ë‹¹ì„± ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¬¼ì„ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì¶œë ¥í•˜ëŠ” ì½”ë“œ

plt.imshow(LRP_out.reshape(28, 28), interpolation="nearest", cmap=plt.cm.jet)

# ì˜ˆì œ 7.13 í•©ì„±ê³± ì‹ ê²½ë§ ì „ì²´ì— ëŒ€í•´ LRPë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ

def getLRP(img):
    predict = sess.run(out_y, feed_dict={x: img.reshape([-1, 784]), keep_prob:1.0})[0]
    layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]
    weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*weights.*')
    biases = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='.*biases.*')

    # layers = [hidden_1, pool_1, hidden_2, pool_2, hidden_3]
    activations = []
    for layer in layers:
        activations.append(getActivations(layer, img))

    # get f_x
    f_x = max(predict)

    # get R4
    predict[predict < 0] = 0
    R4 = predict

    # get R3
    a = activations.pop()
    w = weights.pop()
    b = biases.pop()
    R3 = backprop_dense(a.reshape(1,980), w, b, R4)

    # get R2
    w = weights.pop()
    b = biases.pop()
    p = activations.pop()
    a = activations.pop()
    R_conv = backprop_conv(p, w, b, tf.reshape(R3, [1, 7, 7, 20]))
    R2 = backprop_pooling(a, R_conv)

    # get R1
    w = weights.pop()
    b = biases.pop()
    p = activations.pop()
    a = activations.pop()
    R_conv = backprop_conv(p, w, b, R2)
    R1 = backprop_pooling(a, R_conv)

    # get R0
    img_activations = getActivations(x_image, img)
    w = weights.pop()
    b = biases.pop()
    R0 = backprop_conv(img_activations, w, b, R1)
    LRP_out = sess.run(R0)
    return LRP_out

# ì˜ˆì œ 7.13 í•©ì„±ê³± ì‹ ê²½ë§ ì „ì²´ì— ëŒ€í•´ LRPë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ

# get MNIST dataset index dict
mnist_dict = {}
idx = 0
for i in mnist.test.labels:
    label = np.where(i == np.amax(i))[0][0]
    if mnist_dict.get(label):
        mnist_dict[label].append(idx)
    else:
        mnist_dict[label] = [idx]
    idx += 1

# get LRP
nums = []
for i in range(10):
    img_idx = mnist_dict[i][0]
    img = mnist.test.images[img_idx]
    lrp = getLRP(img)
    nums.append(lrp)

# plot images
plt.figure(figsize=(20,10))
for i in range(2):
    for j in range(5):
        idx = 5 * i + j
        plt.subplot(2, 5, idx + 1)
        plt.title('digit: {}'.format(idx))
        plt.imshow(nums[idx].reshape([28, 28]), cmap=plt.cm.jet)
        plt.colorbar(orientation='horizontal')
plt.tight_layout()
sess.close()